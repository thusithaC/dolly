{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T02:54:26.297042942Z",
     "start_time": "2023-05-17T02:54:22.776564522Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "DOLLY_MODEL = \"databricks/dolly-v2-3b\"\n",
    "EMBEDDING_MODEL = \"all-mpnet-base-v2\"\n",
    "\n",
    "USE_DOLLY_FOR_EMBEDDING = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T02:55:57.831236265Z",
     "start_time": "2023-05-17T02:55:35.320755289Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_text = pipeline(model=DOLLY_MODEL, torch_dtype=torch.bfloat16,\n",
    "                         trust_remote_code=True, device_map=\"auto\", return_full_text=True)\n",
    "# template for an instruction with input\n",
    "prompt_with_context = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"context\"],\n",
    "    template=\"{instruction}\\n\\nInput:\\n{context}\")\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "llm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/databricks_dolly-v2-3b. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/databricks_dolly-v2-3b were not used when initializing GPTNeoXModel: ['embed_out.weight']\n",
      "- This IS expected if you are initializing GPTNeoXModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTNeoXModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if USE_DOLLY_FOR_EMBEDDING:\n",
    "    embedding_model = SentenceTransformer(DOLLY_MODEL)\n",
    "else:\n",
    "    embedding_model = SentenceTransformer(EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T02:56:11.793870686Z",
     "start_time": "2023-05-17T02:56:11.781941764Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_chunk</th>\n",
       "      <th>title</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>embeddings_dolly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Texture Synthesis Using Convolutional NeuralNe...</td>\n",
       "      <td>Texture Synthesis Using Convolutional Neural N...</td>\n",
       "      <td>[0.01643215, 0.08131, -0.051866785, 0.07214568...</td>\n",
       "      <td>[-0.4477233, 0.7085075, 0.9448252, -0.6605596,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 Convolutional neural network We use the VGG...</td>\n",
       "      <td>Texture Synthesis Using Convolutional Neural N...</td>\n",
       "      <td>[0.00494202, -0.0018913345, -6.29354e-05, 0.09...</td>\n",
       "      <td>[-0.16315894, 1.1724734, 0.34991983, -0.975609...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 \f",
       "different features. These feature correlati...</td>\n",
       "      <td>Texture Synthesis Using Convolutional Neural N...</td>\n",
       "      <td>[0.040369663, 0.036883876, -0.026579408, 0.090...</td>\n",
       "      <td>[-0.5808077, 0.93010116, 0.35125256, -0.081901...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conv1_1pool1pool2pool3pool4originalPortilla &amp;...</td>\n",
       "      <td>Texture Synthesis Using Convolutional Neural N...</td>\n",
       "      <td>[-0.005054468, -0.011686467, -0.05832806, 0.06...</td>\n",
       "      <td>[-1.1286112, 0.94930226, 0.83858913, -0.496184...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6 \f",
       "Classification performance 1.00.80.60.4 top...</td>\n",
       "      <td>Texture Synthesis Using Convolutional Neural N...</td>\n",
       "      <td>[0.02261693, -0.04751408, -0.008524225, 0.0593...</td>\n",
       "      <td>[-0.7846361, -0.06361116, 1.3205526, 1.9582235...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_chunk   \n",
       "0  Texture Synthesis Using Convolutional NeuralNe...  \\\n",
       "1  \n",
       "2 Convolutional neural network We use the VGG...   \n",
       "2  3 \n",
       "different features. These feature correlati...   \n",
       "3  \n",
       "conv1_1pool1pool2pool3pool4originalPortilla &...   \n",
       "4  6 \n",
       "Classification performance 1.00.80.60.4 top...   \n",
       "\n",
       "                                               title   \n",
       "0  Texture Synthesis Using Convolutional Neural N...  \\\n",
       "1  Texture Synthesis Using Convolutional Neural N...   \n",
       "2  Texture Synthesis Using Convolutional Neural N...   \n",
       "3  Texture Synthesis Using Convolutional Neural N...   \n",
       "4  Texture Synthesis Using Convolutional Neural N...   \n",
       "\n",
       "                                          embeddings   \n",
       "0  [0.01643215, 0.08131, -0.051866785, 0.07214568...  \\\n",
       "1  [0.00494202, -0.0018913345, -6.29354e-05, 0.09...   \n",
       "2  [0.040369663, 0.036883876, -0.026579408, 0.090...   \n",
       "3  [-0.005054468, -0.011686467, -0.05832806, 0.06...   \n",
       "4  [0.02261693, -0.04751408, -0.008524225, 0.0593...   \n",
       "\n",
       "                                    embeddings_dolly  \n",
       "0  [-0.4477233, 0.7085075, 0.9448252, -0.6605596,...  \n",
       "1  [-0.16315894, 1.1724734, 0.34991983, -0.975609...  \n",
       "2  [-0.5808077, 0.93010116, 0.35125256, -0.081901...  \n",
       "3  [-1.1286112, 0.94930226, 0.83858913, -0.496184...  \n",
       "4  [-0.7846361, -0.06361116, 1.3205526, 1.9582235...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_chunks = pd.read_feather(\"../data/paper_extracts_embed.feature\")\n",
    "df_text_chunks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T02:56:15.173063763Z",
     "start_time": "2023-05-17T02:56:14.217409313Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sentence_transformers/util.py:61: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  b = torch.tensor(b)\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the goal of visual texture synthesis?\"\n",
    "query_emb = embedding_model.encode(query)\n",
    "if USE_DOLLY_FOR_EMBEDDING:\n",
    "    doc_emb = df_text_chunks[\"embeddings_dolly\"]\n",
    "else:\n",
    "    doc_emb = df_text_chunks[\"embeddings\"]\n",
    "\n",
    "docs = df_text_chunks[\"text_chunk\"]\n",
    "scores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n",
    "\n",
    "#Combine docs & scores\n",
    "doc_score_pairs = list(zip(docs, scores))\n",
    "\n",
    "#Sort by decreasing score\n",
    "doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T02:53:13.972437284Z",
     "start_time": "2023-05-17T02:53:13.970789443Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\x0c45 30 15 0 15 30 45 45 30 15 0 15 30 45 Figure 3: 3D view synthesis on Multi-PIE. For each panel, the first row shows the ground truth from 45to 45 , the second and third rows show the re-renderings of 6-step clockwise rotation from an input image of45 (red box) and of 6-step counter-clockwise rotation from an input image of 45 (red box), respectively.45 30 15 15 30 45 45 30 15 15 30 45 InputRNN3DmodelFigure 4: Comparing face pose normalization results with 3D morphable model [29].',\n",
       "  2187.7333984375),\n",
       " ('Performance Comparisons of the Different mQA Variants In order to show the effectiveness of the different components and strategies of our mQA model, weimplement three variants of the mQA in Figure 2. For the first variant (i.e. mQA-avg-question), wereplace the first LSTM component of the model (i.e. the LSTM to extract the question embedding) Image QuestionAnswer What is in the plate? food. What is the dog doing? Surfing in the sea. Where is the cat? On the bed. What is there in the image? There is a clock. What is the type of the vehicle? Train. Figure 4: Random examples of the answers generated by the mQA model with score 1 given bythe human judges.7 \\x0cImage GeneratedQuestionAnswer Where is this? Is this guy playing tennis? What kind of food is this? Where is the computer? This is the kitchen room. Yes. Pizza. On the desk. Figure 5: The sample generated questions by our model and their answers.with the average embedding of the words in the quesWord Error Losstion using word2vec [29]. It is used to show the effecmQA-avg-question0.4422.17mQA-same-LSTMs0.4392.09tiveness of the LSTM as a question embedding learnermQA-noTWS0.4382.14and extractor. For the second variant (i.e. mQAmQA-complete0.3931.91same-LSTMs), we use two shared-weights LSTMs tomodel question and answer. It is used to show the effectiveness of the decoupling strategy of the weights of Table 2: Performance comparisons of thethe LSTM(Q) and the LSTM(A) in our model. For the different mQA variants.third variant (i.e. mQA-noTWS), we do not adopt the Transposed Weight Sharing (TWS) strategy.It is used to show the effectiveness of TWS.The word error rates and losses of the three variants and the complete mQA model (i.e. mQAcomplete) are shown in Table 2. All of the three variants performs worse than our mQA model. 6 Discussion',\n",
       "  2005.492919921875),\n",
       " ('Image QuestionGT AnswermQA Answer     What is the handsome boy doing? What is there in the image?  Which fruit is there in the plate? What is the type of the vehicle? Why does the bus park there?        Trying to catch the frisbee. Horses on the grassland. Apples and oranges. Bus. Preparing for repair.       <OOV> Surfing. They are buffalos. Bananas and oranges. Train. <OOV> (I do not know.) Figure 6: Failure cases of our mQA model on the FM-IQA dataset.8',\n",
       "  1927.5791015625)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_score_pairs[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:02:33.290401171Z",
     "start_time": "2023-05-17T02:56:20.809691710Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main goal of visual texture synthesis is to generate high-quality images to answer given a text description. For example, given a sentence “a grey cat is running behind a red car”, the goal of visual texture synthesis is to generate an image with cat and car.\n",
      "\n",
      "We can see that the first variant (i.e. mQA-avg-question) is not good at generating image. It generates random examples with the answers given by the human judges. We can see that our mQA model can generate better answers with a lower word error rate.\n",
      "\n",
      "The second variant (i.e. mQA-same-LSTMs) is much better than the first variant. We can see that the first variant uses the LSTM to extract the question embedding from the input sentence. It learns the sentence word pattern. It is much worse than the second variant.\n"
     ]
    }
   ],
   "source": [
    "context_chunks = \"\\n\\n\".join([doc_score_pairs[i][0] for i in [1]])\n",
    "print(llm_context_chain.predict(instruction=query, context=context_chunks).lstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
