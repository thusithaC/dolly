{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T02:54:26.297042942Z",
     "start_time": "2023-05-17T02:54:22.776564522Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "DOLLY_MODEL = \"databricks/dolly-v2-3b\"\n",
    "EMBEDDING_MODEL = \"all-mpnet-base-v2\"\n",
    "\n",
    "USE_DOLLY_FOR_EMBEDDING = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T02:55:57.831236265Z",
     "start_time": "2023-05-17T02:55:35.320755289Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_text = pipeline(model=DOLLY_MODEL, torch_dtype=torch.bfloat16,\n",
    "                         trust_remote_code=True, device_map=\"auto\", return_full_text=True, do_sample=False)\n",
    "# template for an instruction with input\n",
    "prompt_with_context = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"context\"],\n",
    "    template=\"{instruction}\\n\\nInput:\\n{context}\")\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "llm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "if USE_DOLLY_FOR_EMBEDDING:\n",
    "    embedding_model = SentenceTransformer(DOLLY_MODEL)\n",
    "else:\n",
    "    embedding_model = SentenceTransformer(EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T02:56:11.793870686Z",
     "start_time": "2023-05-17T02:56:11.781941764Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_chunk</th>\n",
       "      <th>title</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>embeddings_dolly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Texture Synthesis Using Convolutional NeuralNe...</td>\n",
       "      <td>Texture Synthesis Using Convolutional Neural N...</td>\n",
       "      <td>[0.01643215, 0.08131, -0.051866785, 0.07214568...</td>\n",
       "      <td>[-0.4477233, 0.7085075, 0.9448252, -0.6605596,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 Convolutional neural network We use the VGG...</td>\n",
       "      <td>Texture Synthesis Using Convolutional Neural N...</td>\n",
       "      <td>[0.00494202, -0.0018913345, -6.29354e-05, 0.09...</td>\n",
       "      <td>[-0.16315894, 1.1724734, 0.34991983, -0.975609...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 \f",
       "different features. These feature correlati...</td>\n",
       "      <td>Texture Synthesis Using Convolutional Neural N...</td>\n",
       "      <td>[0.040369663, 0.036883876, -0.026579408, 0.090...</td>\n",
       "      <td>[-0.5808077, 0.93010116, 0.35125256, -0.081901...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conv1_1pool1pool2pool3pool4originalPortilla &amp;...</td>\n",
       "      <td>Texture Synthesis Using Convolutional Neural N...</td>\n",
       "      <td>[-0.005054468, -0.011686467, -0.05832806, 0.06...</td>\n",
       "      <td>[-1.1286112, 0.94930226, 0.83858913, -0.496184...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6 \f",
       "Classification performance 1.00.80.60.4 top...</td>\n",
       "      <td>Texture Synthesis Using Convolutional Neural N...</td>\n",
       "      <td>[0.02261693, -0.04751408, -0.008524225, 0.0593...</td>\n",
       "      <td>[-0.7846361, -0.06361116, 1.3205526, 1.9582235...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_chunk   \n",
       "0  Texture Synthesis Using Convolutional NeuralNe...  \\\n",
       "1  \n",
       "2 Convolutional neural network We use the VGG...   \n",
       "2  3 \n",
       "different features. These feature correlati...   \n",
       "3  \n",
       "conv1_1pool1pool2pool3pool4originalPortilla &...   \n",
       "4  6 \n",
       "Classification performance 1.00.80.60.4 top...   \n",
       "\n",
       "                                               title   \n",
       "0  Texture Synthesis Using Convolutional Neural N...  \\\n",
       "1  Texture Synthesis Using Convolutional Neural N...   \n",
       "2  Texture Synthesis Using Convolutional Neural N...   \n",
       "3  Texture Synthesis Using Convolutional Neural N...   \n",
       "4  Texture Synthesis Using Convolutional Neural N...   \n",
       "\n",
       "                                          embeddings   \n",
       "0  [0.01643215, 0.08131, -0.051866785, 0.07214568...  \\\n",
       "1  [0.00494202, -0.0018913345, -6.29354e-05, 0.09...   \n",
       "2  [0.040369663, 0.036883876, -0.026579408, 0.090...   \n",
       "3  [-0.005054468, -0.011686467, -0.05832806, 0.06...   \n",
       "4  [0.02261693, -0.04751408, -0.008524225, 0.0593...   \n",
       "\n",
       "                                    embeddings_dolly  \n",
       "0  [-0.4477233, 0.7085075, 0.9448252, -0.6605596,...  \n",
       "1  [-0.16315894, 1.1724734, 0.34991983, -0.975609...  \n",
       "2  [-0.5808077, 0.93010116, 0.35125256, -0.081901...  \n",
       "3  [-1.1286112, 0.94930226, 0.83858913, -0.496184...  \n",
       "4  [-0.7846361, -0.06361116, 1.3205526, 1.9582235...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_chunks = pd.read_feather(\"../data/paper_extracts_embed.feature\")\n",
    "df_text_chunks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T02:56:15.173063763Z",
     "start_time": "2023-05-17T02:56:14.217409313Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_similarity(embedding_model, query, doc_emb, docs):\n",
    "    query_emb = embedding_model.encode(query)\n",
    "    scores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n",
    "    #Combine docs & scores\n",
    "    doc_score_pairs = list(zip(docs, scores))\n",
    "    #Sort by decreasing score\n",
    "    doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    return doc_score_pairs\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T02:53:13.972437284Z",
     "start_time": "2023-05-17T02:53:13.970789443Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------With Context--------------------\n",
      "\n",
      "Gram matrices are used to measure the similarity between feature responses of different layers of a convolutional neural network. They are used in the context of texture synthesis to find a generating process that maps a source texture to another.\n",
      "\n",
      "----------------Without Context--------------------\n",
      "\n",
      "A Gram matrix is a type of matrix that is used in linear algebra to represent a inner product between vectors.  The inner product is computed from a set of vectors, called a basis, and a matrix that represents the basis.  The Gram matrix is used to represent the linear relationship between the basis vectors.  For example, the inner product of two vectors can be represented by the dot product of the corresponding vectors.  This dot product is computed by first applying the corresponding basis matrix to the two vectors.  The result of this computation is a number.  This number is called the inner product of the two vectors.  The inner product of two vectors can also be represented by a dot product of the two Gram matrices of the basis vectors.  The Gram matrix of a basis is a square matrix with the same dimensions as the basis.  The inner product of two vectors is the dot product of the corresponding vectors and the basis.  The inner product of two vectors can also be represented by a dot product of the corresponding Gram matrices.\n"
     ]
    }
   ],
   "source": [
    "if USE_DOLLY_FOR_EMBEDDING:\n",
    "    doc_emb = df_text_chunks[\"embeddings_dolly\"]\n",
    "else:\n",
    "    doc_emb = df_text_chunks[\"embeddings\"]\n",
    "\n",
    "def get_context(query, docs = df_text_chunks[\"text_chunk\"], doc_emb=doc_emb, embedding_model=embedding_model):\n",
    "    doc_score_pairs = get_similarity(embedding_model, query, doc_emb, docs)\n",
    "    context_chunks = \"\\n\\n\".join([doc_score_pairs[i][0] for i in [1]])\n",
    "    return context_chunks\n",
    "\n",
    "\n",
    "query = [\"What is the goal of visual texture synthesis?\", \"what is VGG-19\", \"What are Gram matrices used for\"][2]\n",
    "context = get_context(query=query)\n",
    "\n",
    "print(\"\\n----------------With Context--------------------\\n\")\n",
    "print(llm_context_chain.predict(instruction=query, context=context_chunks).lstrip())\n",
    "\n",
    "print(\"\\n----------------Without Context--------------------\\n\")\n",
    "print(llm_context_chain.predict(instruction=query, context=\"\").lstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:02:33.290401171Z",
     "start_time": "2023-05-17T02:56:20.809691710Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3 \\x0cdifferent features. These feature correlations are, up to a constant of proportionality, given by theGram matrix Gl  RNl Nl , where Glij is the inner product between feature map i and j in layer l:XllGlij =FikFjk.(1)k1 2 L A set of Gram matrices {G , G , ..., G } from some layers 1, . . . , L in the network in response toa given texture provides a stationary description of the texture, which fully specifies a texture in ourmodel (Fig. 1A). 4 Texture generation To generate a new texture on the basis of a given image, we use gradient descent from a white noiseimage to find another image that matches the Gram-matrix representation of the original image.This optimisation is done by minimising the mean-squared distance between the entries of the Grammatrix of the original image and the Gram matrix of the image being generated (Fig. 1B). be the original image and the image that is generated, and Gl and Gl their respectiveLet ~x and ~xGram-matrix representations in layer l (Eq. 1). The contribution of layer l to the total loss is then\\x112X\\x101El =Glij  Glij(2)224Nl Ml i,jand the total loss is) =L(~x, ~x LX wl El (3) l=0 where wl are weighting factors of the contribution of each layer to the total loss. The derivative ofEl with respect to the activations in layer l can be computed analytically:\\x10\\x10\\x11\\x11(1l Tll(F)G Gif Fijl > 0El22Nl Mlji=(4) Fijl0if Fijl < 0 .), with respect to the pixels ~x can be readilyThe gradients of El , and thus the gradient of L(~x, ~xLcomputed using standard error back-propagation [18]. The gradient  ~x can be used as input forsome numerical optimisation strategy. In our work we use L-BFGS [30], which seemed a reasonablechoice for the high-dimensional optimisation problem at hand. The entire procedure relies mainlyon the standard forward-backward pass that is used to train the convolutional network. Therefore, inspite of the large complexity of the model, texture generation can be done in reasonable time usingGPUs and performance-optimised toolboxes for training deep neural networks [12]. 5 Results'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1080: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Gram matrix is a type of matrix that is used in linear algebra to represent a inner product between vectors.  The inner product is computed from a set of vectors, called a basis, and a matrix that represents the basis.  The Gram matrix is used to represent the linear relationship between the basis vectors.  For example, the inner product of two vectors can be represented by the dot product of the corresponding vectors.  This dot product is computed by first applying the corresponding basis matrix to the two vectors.  The result of this computation is a number.  This number is called the inner product of the two vectors.  The inner product of two vectors can also be represented by a dot product of the two Gram matrices of the basis vectors.  The Gram matrix of a basis is a square matrix with the same dimensions as the basis.  The inner product of two vectors is the dot product of the corresponding vectors and the basis.  The inner product of two vectors can also be represented by a dot product of the corresponding Gram matrices.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
