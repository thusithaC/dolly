{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-05-17T06:11:40.492150936Z",
     "start_time": "2023-05-17T06:11:40.489529467Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "DOLLY_MODEL = \"databricks/dolly-v2-3b\"\n",
    "EMBEDDING_MODEL = [\"all-mpnet-base-v2\", \"sentence-transformers/use-cmlm-multilingual\"][1]\n",
    "\n",
    "USE_DOLLY_FOR_EMBEDDING = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T02:55:57.831236265Z",
     "start_time": "2023-05-17T02:55:35.320755289Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_text = pipeline(model=DOLLY_MODEL, torch_dtype=torch.bfloat16,\n",
    "                         trust_remote_code=True, device_map=\"auto\", return_full_text=True, do_sample=False)\n",
    "# template for an instruction with input\n",
    "prompt_with_context = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"context\"],\n",
    "    template=\"{instruction}\\n\\nInput:\\n{context}\")\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "llm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-05-17T06:11:47.894098337Z",
     "start_time": "2023-05-17T06:11:44.425407600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/thusitha/.cache/torch/sentence_transformers/sentence-transformers_use-cmlm-multilingual/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if USE_DOLLY_FOR_EMBEDDING:\n",
    "    embedding_model = SentenceTransformer(DOLLY_MODEL)\n",
    "else:\n",
    "    embedding_model = SentenceTransformer(EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-05-17T06:13:52.452757006Z",
     "start_time": "2023-05-17T06:13:52.411000617Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                          text_chunk   \n0  Texture Synthesis Using Convolutional NeuralNe...  \\\n1  \n2 Convolutional neural network We use the VGG...   \n2  3 \ndifferent features. These feature correlati...   \n3  \nconv1_1pool1pool2pool3pool4originalPortilla &...   \n4  6 \nClassification performance 1.00.80.60.4 top...   \n\n                                               title   \n0  Texture Synthesis Using Convolutional Neural N...  \\\n1  Texture Synthesis Using Convolutional Neural N...   \n2  Texture Synthesis Using Convolutional Neural N...   \n3  Texture Synthesis Using Convolutional Neural N...   \n4  Texture Synthesis Using Convolutional Neural N...   \n\n                                          embeddings   \n0  [0.01643215, 0.08131, -0.051866785, 0.07214568...  \\\n1  [0.00494202, -0.0018913345, -6.29354e-05, 0.09...   \n2  [0.040369663, 0.036883876, -0.026579408, 0.090...   \n3  [-0.005054468, -0.011686467, -0.05832806, 0.06...   \n4  [0.02261693, -0.04751408, -0.008524225, 0.0593...   \n\n                                    embeddings_dolly   \n0  [-0.4477233, 0.7085075, 0.9448252, -0.6605596,...  \\\n1  [-0.16315894, 1.1724734, 0.34991983, -0.975609...   \n2  [-0.5808077, 0.93010116, 0.35125256, -0.081901...   \n3  [-1.1286112, 0.94930226, 0.83858913, -0.496184...   \n4  [-0.7846361, -0.06361116, 1.3205526, 1.9582235...   \n\n                                      embeddings_use  \n0  [-0.11053032, 0.036556076, 0.029679747, 0.0063...  \n1  [-0.062598296, 0.03323136, -0.0032446731, -0.0...  \n2  [-0.060262118, 0.016366797, 0.020364584, 0.033...  \n3  [-0.07421292, -0.0026081826, 0.03418953, 0.043...  \n4  [-0.04890534, -0.0038033319, 0.05069413, 0.020...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_chunk</th>\n      <th>title</th>\n      <th>embeddings</th>\n      <th>embeddings_dolly</th>\n      <th>embeddings_use</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Texture Synthesis Using Convolutional NeuralNe...</td>\n      <td>Texture Synthesis Using Convolutional Neural N...</td>\n      <td>[0.01643215, 0.08131, -0.051866785, 0.07214568...</td>\n      <td>[-0.4477233, 0.7085075, 0.9448252, -0.6605596,...</td>\n      <td>[-0.11053032, 0.036556076, 0.029679747, 0.0063...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2 Convolutional neural network We use the VGG...</td>\n      <td>Texture Synthesis Using Convolutional Neural N...</td>\n      <td>[0.00494202, -0.0018913345, -6.29354e-05, 0.09...</td>\n      <td>[-0.16315894, 1.1724734, 0.34991983, -0.975609...</td>\n      <td>[-0.062598296, 0.03323136, -0.0032446731, -0.0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3 \fdifferent features. These feature correlati...</td>\n      <td>Texture Synthesis Using Convolutional Neural N...</td>\n      <td>[0.040369663, 0.036883876, -0.026579408, 0.090...</td>\n      <td>[-0.5808077, 0.93010116, 0.35125256, -0.081901...</td>\n      <td>[-0.060262118, 0.016366797, 0.020364584, 0.033...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>conv1_1pool1pool2pool3pool4originalPortilla &amp;...</td>\n      <td>Texture Synthesis Using Convolutional Neural N...</td>\n      <td>[-0.005054468, -0.011686467, -0.05832806, 0.06...</td>\n      <td>[-1.1286112, 0.94930226, 0.83858913, -0.496184...</td>\n      <td>[-0.07421292, -0.0026081826, 0.03418953, 0.043...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6 \fClassification performance 1.00.80.60.4 top...</td>\n      <td>Texture Synthesis Using Convolutional Neural N...</td>\n      <td>[0.02261693, -0.04751408, -0.008524225, 0.0593...</td>\n      <td>[-0.7846361, -0.06361116, 1.3205526, 1.9582235...</td>\n      <td>[-0.04890534, -0.0038033319, 0.05069413, 0.020...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_chunks = pd.read_feather(\"../data/paper_extracts_embed.feature\")\n",
    "df_text_chunks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-05-17T06:13:55.575194225Z",
     "start_time": "2023-05-17T06:13:55.571007276Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_similarity(embedding_model, query, doc_emb, docs):\n",
    "    query_emb = embedding_model.encode(query)\n",
    "    scores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n",
    "    #Combine docs & scores\n",
    "    doc_score_pairs = list(zip(docs, scores))\n",
    "    #Sort by decreasing score\n",
    "    doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    return doc_score_pairs\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-05-17T06:17:33.890748277Z",
     "start_time": "2023-05-17T06:17:33.863362577Z"
    }
   },
   "outputs": [],
   "source": [
    "if USE_DOLLY_FOR_EMBEDDING:\n",
    "    doc_emb = df_text_chunks[\"embeddings_dolly\"]\n",
    "else:\n",
    "    if EMBEDDING_MODEL == \"sentence-transformers/use-cmlm-multilingual\":\n",
    "        doc_emb = df_text_chunks[\"embeddings_use\"]\n",
    "    else:\n",
    "        doc_emb = df_text_chunks[\"embeddings\"]\n",
    "\n",
    "def get_context(query, top_n = 3, docs = df_text_chunks[\"text_chunk\"], doc_emb=doc_emb, embedding_model=embedding_model):\n",
    "    doc_score_pairs = get_similarity(embedding_model, query, doc_emb, docs)\n",
    "    context_chunks = \"\\n\\n\".join([doc_score_pairs[i][0] for i in range(top_n)])\n",
    "    return context_chunks\n",
    "\n",
    "\n",
    "query = [\"What is the goal of visual texture synthesis?\", \"what is VGG-19\", \"What are Gram matrices used for\"][0]\n",
    "context = get_context(query=query, top_n=2)\n",
    "\n",
    "print(\"\\n----------------With Context--------------------\\n\")\n",
    "print(llm_context_chain.predict(instruction=query, context=context).lstrip())\n",
    "\n",
    "print(\"\\n----------------Without Context--------------------\\n\")\n",
    "print(llm_context_chain.predict(instruction=query, context=\"\").lstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-05-17T06:17:36.786115672Z",
     "start_time": "2023-05-17T06:17:36.777996011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Texture Synthesis Using Convolutional NeuralNetworksLeon A. GatysCentre for Integrative Neuroscience, University of Tubingen, GermanyBernstein Center for Computational Neuroscience, Tubingen, GermanyGraduate School of Neural Information Processing, University of Tubingen, Germanyleon.gatys@bethgelab.orgAlexander S. EckerCentre for Integrative Neuroscience, University of Tubingen, GermanyBernstein Center for Computational Neuroscience, Tubingen, GermanyMax Planck Institute for Biological Cybernetics, Tubingen, GermanyBaylor College of Medicine, Houston, TX, USAMatthias BethgeCentre for Integrative Neuroscience, University of Tubingen, GermanyBernstein Center for Computational Neuroscience, Tubingen, GermanyMax Planck Institute for Biological Cybernetics, Tubingen, Germany AbstractHere we introduce a new model of natural textures based on the feature spacesof convolutional neural networks optimised for object recognition. Samples fromthe model are of high perceptual quality demonstrating the generative power ofneural networks trained in a purely discriminative fashion. Within the model, textures are represented by the correlations between feature maps in several layers ofthe network. We show that across layers the texture representations increasinglycapture the statistical properties of natural images while making object information more and more explicit. The model provides a new tool to generate stimulifor neuroscience and might offer insights into the deep representations learned byconvolutional neural networks. 1 Introduction The goal of visual texture synthesis is to infer a generating process from an example texture, whichthen allows to produce arbitrarily many new samples of that texture. The evaluation criterion for thequality of the synthesised texture is usually human inspection and textures are successfully synthesised if a human observer cannot tell the original texture from a synthesised one.In general, there are two main approaches to find a texture generating process. The first approach isto generate a new texture by resampling either pixels [5, 28] or whole patches [6, 16] of the originaltexture. These non-parametric resampling techniques and their numerous extensions and improvements (see [27] for review) are capable of producing high quality natural textures very efficiently.However, they do not define an actual model for natural textures but rather give a mechanistic procedure for how one can randomise a source texture without changing its perceptual properties.In contrast, the second approach to texture synthesis is to explicitly define a parametric texturemodel. The model usually consists of a set of statistical measurements that are taken over the1 \\x0c1 512... 4 conv5_ 3 21 1 pool4 512... 4 conv4_ 3 21 1 256... pool34 conv3_ 3 21 pool21 1 128... 64... # featuremaps conv2_ 1 2pool1conv1_ 21 Gradientdescent input Figure 1: Synthesis method. Texture analysis (left). The original texture is passed through the CNNand the Gram matrices Gl on the feature responses of a number of layers are computed. Texture is passed through the CNN and a loss function El issynthesis (right). A white noise image ~xcomputed on every layer included in the texture model. The total loss function L is a weighted sumof the contributions El from each layer. Using gradient descent on the total loss with respect to thepixel values, a new image is found that produces the same Gram matrices Gl as the original texture.'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1080: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Gram matrix is a type of matrix that is used in linear algebra to represent a inner product between vectors.  The inner product is computed from a set of vectors, called a basis, and a matrix that represents the basis.  The Gram matrix is used to represent the linear relationship between the basis vectors.  For example, the inner product of two vectors can be represented by the dot product of the corresponding vectors.  This dot product is computed by first applying the corresponding basis matrix to the two vectors.  The result of this computation is a number.  This number is called the inner product of the two vectors.  The inner product of two vectors can also be represented by a dot product of the two Gram matrices of the basis vectors.  The Gram matrix of a basis is a square matrix with the same dimensions as the basis.  The inner product of two vectors is the dot product of the corresponding vectors and the basis.  The inner product of two vectors can also be represented by a dot product of the corresponding Gram matrices.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
