{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T06:11:40.492150936Z",
     "start_time": "2023-05-17T06:11:40.489529467Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "DOLLY_MODEL = \"databricks/dolly-v2-12b\"\n",
    "EMBEDDING_MODEL = [\"all-mpnet-base-v2\", \"sentence-transformers/use-cmlm-multilingual\"][1]\n",
    "\n",
    "USE_DOLLY_FOR_EMBEDDING = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T02:55:57.831236265Z",
     "start_time": "2023-05-17T02:55:35.320755289Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f916723b5ca4836bfb97d1e24556469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd284b465688486c8e55f2c6dfde6bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)instruct_pipeline.py:   0%|          | 0.00/9.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/databricks/dolly-v2-12b:\n",
      "- instruct_pipeline.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e90def03614cd7af6586fd4bab2df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/23.8G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b2c07691f749e1a4dfee7e09e46d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/449 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f879b0f4de47df978691aaa1dc1e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54695de771ee4a4282903beb5836fd05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_text = pipeline(model=DOLLY_MODEL, torch_dtype=torch.bfloat16,\n",
    "                         trust_remote_code=True, device_map=\"auto\", return_full_text=True, do_sample=False)\n",
    "# template for an instruction with input\n",
    "prompt_with_context = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"context\"],\n",
    "    template=\"{instruction}\\n\\nInput:\\n{context}\")\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "llm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T06:11:47.894098337Z",
     "start_time": "2023-05-17T06:11:44.425407600Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a9d970777a4068aecba1202afee58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7729f/.gitattributes:   0%|          | 0.00/1.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee87cb1a54ee4029833d208f9f0bcdd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795309d16c6d464f84e54fc2940b4504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)a63d77729f/README.md:   0%|          | 0.00/1.85k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3bf0eeb0a64972a1f560969e3de756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)3d77729f/config.json:   0%|          | 0.00/804 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba2e46a5f4b43bf8df3fabd1bab552d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac01391c95e94b879e4b548b1960ed68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbfcadb54b474e5ca7f70086e3ae4f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c0381281d24ef4a3752319b6af7a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d032f4d9c6df435baa8b80af25666bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7729f/tokenizer.json:   0%|          | 0.00/9.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcaf58ffed4a4fa2af6388ff5bdb75cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386b127744a44a3581eeec42fd71f786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)a63d77729f/vocab.txt:   0%|          | 0.00/5.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d11cfab29d47fc963385ab0fd083ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)d77729f/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/sentence-transformers_use-cmlm-multilingual/ were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if USE_DOLLY_FOR_EMBEDDING:\n",
    "    embedding_model = SentenceTransformer(DOLLY_MODEL)\n",
    "else:\n",
    "    embedding_model = SentenceTransformer(EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T06:13:52.452757006Z",
     "start_time": "2023-05-17T06:13:52.411000617Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_chunk</th>\n",
       "      <th>title</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>embeddings_dolly</th>\n",
       "      <th>embeddings_use</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Texture Synthesis Using Convolutional NeuralNe...</td>\n",
       "      <td>Texture Synthesis Using Convolutional Neural N...</td>\n",
       "      <td>[0.01643215, 0.08131, -0.051866785, 0.07214568...</td>\n",
       "      <td>[-0.4477233, 0.7085075, 0.9448252, -0.6605596,...</td>\n",
       "      <td>[-0.11053032, 0.036556076, 0.029679747, 0.0063...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 Convolutional neural network We use the VGG...</td>\n",
       "      <td>Texture Synthesis Using Convolutional Neural N...</td>\n",
       "      <td>[0.00494202, -0.0018913345, -6.29354e-05, 0.09...</td>\n",
       "      <td>[-0.16315894, 1.1724734, 0.34991983, -0.975609...</td>\n",
       "      <td>[-0.062598296, 0.03323136, -0.0032446731, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 \f",
       "different features. These feature correlati...</td>\n",
       "      <td>Texture Synthesis Using Convolutional Neural N...</td>\n",
       "      <td>[0.040369663, 0.036883876, -0.026579408, 0.090...</td>\n",
       "      <td>[-0.5808077, 0.93010116, 0.35125256, -0.081901...</td>\n",
       "      <td>[-0.060262118, 0.016366797, 0.020364584, 0.033...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conv1_1pool1pool2pool3pool4originalPortilla &amp;...</td>\n",
       "      <td>Texture Synthesis Using Convolutional Neural N...</td>\n",
       "      <td>[-0.005054468, -0.011686467, -0.05832806, 0.06...</td>\n",
       "      <td>[-1.1286112, 0.94930226, 0.83858913, -0.496184...</td>\n",
       "      <td>[-0.07421292, -0.0026081826, 0.03418953, 0.043...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6 \f",
       "Classification performance 1.00.80.60.4 top...</td>\n",
       "      <td>Texture Synthesis Using Convolutional Neural N...</td>\n",
       "      <td>[0.02261693, -0.04751408, -0.008524225, 0.0593...</td>\n",
       "      <td>[-0.7846361, -0.06361116, 1.3205526, 1.9582235...</td>\n",
       "      <td>[-0.04890534, -0.0038033319, 0.05069413, 0.020...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_chunk   \n",
       "0  Texture Synthesis Using Convolutional NeuralNe...  \\\n",
       "1  \n",
       "2 Convolutional neural network We use the VGG...   \n",
       "2  3 \n",
       "different features. These feature correlati...   \n",
       "3  \n",
       "conv1_1pool1pool2pool3pool4originalPortilla &...   \n",
       "4  6 \n",
       "Classification performance 1.00.80.60.4 top...   \n",
       "\n",
       "                                               title   \n",
       "0  Texture Synthesis Using Convolutional Neural N...  \\\n",
       "1  Texture Synthesis Using Convolutional Neural N...   \n",
       "2  Texture Synthesis Using Convolutional Neural N...   \n",
       "3  Texture Synthesis Using Convolutional Neural N...   \n",
       "4  Texture Synthesis Using Convolutional Neural N...   \n",
       "\n",
       "                                          embeddings   \n",
       "0  [0.01643215, 0.08131, -0.051866785, 0.07214568...  \\\n",
       "1  [0.00494202, -0.0018913345, -6.29354e-05, 0.09...   \n",
       "2  [0.040369663, 0.036883876, -0.026579408, 0.090...   \n",
       "3  [-0.005054468, -0.011686467, -0.05832806, 0.06...   \n",
       "4  [0.02261693, -0.04751408, -0.008524225, 0.0593...   \n",
       "\n",
       "                                    embeddings_dolly   \n",
       "0  [-0.4477233, 0.7085075, 0.9448252, -0.6605596,...  \\\n",
       "1  [-0.16315894, 1.1724734, 0.34991983, -0.975609...   \n",
       "2  [-0.5808077, 0.93010116, 0.35125256, -0.081901...   \n",
       "3  [-1.1286112, 0.94930226, 0.83858913, -0.496184...   \n",
       "4  [-0.7846361, -0.06361116, 1.3205526, 1.9582235...   \n",
       "\n",
       "                                      embeddings_use  \n",
       "0  [-0.11053032, 0.036556076, 0.029679747, 0.0063...  \n",
       "1  [-0.062598296, 0.03323136, -0.0032446731, -0.0...  \n",
       "2  [-0.060262118, 0.016366797, 0.020364584, 0.033...  \n",
       "3  [-0.07421292, -0.0026081826, 0.03418953, 0.043...  \n",
       "4  [-0.04890534, -0.0038033319, 0.05069413, 0.020...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_chunks = pd.read_feather(\"../data/paper_extracts_embed.feature\")\n",
    "df_text_chunks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T06:13:55.575194225Z",
     "start_time": "2023-05-17T06:13:55.571007276Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_similarity(embedding_model, query, doc_emb, docs):\n",
    "    query_emb = embedding_model.encode(query)\n",
    "    scores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n",
    "    #Combine docs & scores\n",
    "    doc_score_pairs = list(zip(docs, scores))\n",
    "    #Sort by decreasing score\n",
    "    doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    return doc_score_pairs\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T06:17:33.890748277Z",
     "start_time": "2023-05-17T06:17:33.863362577Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------With Context--------------------\n",
      "\n",
      "You can start repairs to your vehicle after an accident based on RACV insurance policy if the damage is not too extensive. You should get your vehicle inspected by a qualified mechanic as soon as possible to determine the extent of the damage. It is important to get the accident information, the vehicle information and the insurance information to the mechanic. The insurance information can be found on the RACV website.\n",
      "\n",
      "If the damage to your vehicle is extensive, you will likely be looking at a replacement vehicle.\n",
      "\n",
      "----------------Without Context--------------------\n",
      "\n",
      "If the damage to your vehicle is not too severe, you can start repairs as soon as possible, but not later than 7 days after the accident. However, you should always get at least one estimate of the repair cost before starting the repairs.\n"
     ]
    }
   ],
   "source": [
    "if USE_DOLLY_FOR_EMBEDDING:\n",
    "    doc_emb = df_text_chunks[\"embeddings_dolly\"]\n",
    "else:\n",
    "    if EMBEDDING_MODEL == \"sentence-transformers/use-cmlm-multilingual\":\n",
    "        doc_emb = df_text_chunks[\"embeddings_use\"]\n",
    "    else:\n",
    "        doc_emb = df_text_chunks[\"embeddings\"]\n",
    "\n",
    "def get_context(query, top_n = 3, docs = df_text_chunks[\"text_chunk\"], doc_emb=doc_emb, embedding_model=embedding_model):\n",
    "    doc_score_pairs = get_similarity(embedding_model, query, doc_emb, docs)\n",
    "    context_chunks = \"\\n\\n\".join([doc_score_pairs[i][0] for i in range(top_n)])\n",
    "    return context_chunks\n",
    "\n",
    "\n",
    "query = [\"What is the goal of visual texture synthesis?\",\n",
    "         \"what is VGG-19\",\n",
    "         \"What is a Gram matrix?\",\n",
    "         \"What is a MCGSM\",\n",
    "         \"Describe a spatial LSTM\", \n",
    "         \"what is special about Larochelle & Murray's NADE model?\",\n",
    "         \"What is RACV Years of Membership Benefits program?\", \n",
    "         \"What is insurance excess?\",\n",
    "         \"When can you start repairs to your vehicle after an accident based on RACV insurance policy?\"][-1]\n",
    "context = get_context(query=query, top_n=2)\n",
    "\n",
    "print(\"\\n----------------With Context--------------------\\n\")\n",
    "print(llm_context_chain.predict(instruction=query, context=context).lstrip())\n",
    "\n",
    "print(\"\\n----------------Without Context--------------------\\n\")\n",
    "print(llm_context_chain.predict(instruction=query, context=\"\").lstrip())\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T06:17:36.786115672Z",
     "start_time": "2023-05-17T06:17:36.777996011Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We are actively developing and expanding the dataset, please find the latest information on the project page: http://idl.baidu.com/FM-IQA.html2The results reported in this paper are obtained from a model trained on the first version of the dataset (asubset of the current version) which contains 120,360 images and 250,569 question-answer pairs. 2 \\x0cWhat is the cat doing ? <BOA> Sitting on the umbrella SharedEmbeddingShared LSTM Fusing CNN IntermediateSoftmaxSitting on the umbrella <EOA> Figure 2: Illustration of the mQA model architecture. We input an image and a question about theimage (i.e. What is the cat doing?) to the model. The model is trained to generate the answer tothe question (i.e. Sitting on the umbrella). The weight matrix in the word embedding layers ofthe two LSTMs (one for the question and one for the answer) are shared. In addition, as in [25], thisweight matrix is also shared, in a transposed manner, with the weight matrix in the Softmax layer.Different colors in the figure represent different components of the model. (Best viewed in color.)There are some concurrent and independent works on this topic: [1, 23, 32]. [1] propose a largescale dataset also based on MS COCO. They also provide some simple baseline methods on thisdataset. Compared to them, we propose a stronger model for this task and evaluate our method usinghuman judges. Our dataset also contains two different kinds of language, which can be useful forother tasks, such as machine translation. Because we use a different set of annotators and differentrequirements of the annotation, our dataset and the [1] can be complementary to each other, and leadto some interesting topics, such as dataset transferring for visual question answering.Both [23] and [32] use a model containing a single LSTM and a CNN. They concatenate the questionand the answer (for [32], the answer is a single word. [23] also prefer a single word as the answer),and then feed them to the LSTM. Different from them, we use two separate LSTMs for questionsand answers respectively in consideration of the different properties (e.g. grammar) of questions andanswers, while allow the sharing of the word-embeddings. For the dataset, [23] adopt the datasetproposed in [22], which is much smaller than our FM-IQA dataset. [32] utilize the annotations inMS COCO and synthesize a dataset with four pre-defined types of questions (i.e. object, number,color, and location). They also synthesize the answer with a single word. Their dataset can also becomplementary to ours. 3 The Multimodal QA (mQA) Model\\n\\nImage QuestionGT AnswermQA Answer     What is the handsome boy doing? What is there in the image?  Which fruit is there in the plate? What is the type of the vehicle? Why does the bus park there?        Trying to catch the frisbee. Horses on the grassland. Apples and oranges. Bus. Preparing for repair.       <OOV> Surfing. They are buffalos. Bananas and oranges. Train. <OOV> (I do not know.) Figure 6: Failure cases of our mQA model on the FM-IQA dataset.8'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1080: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Gram matrix is a type of matrix that is used in linear algebra to represent a inner product between vectors.  The inner product is computed from a set of vectors, called a basis, and a matrix that represents the basis.  The Gram matrix is used to represent the linear relationship between the basis vectors.  For example, the inner product of two vectors can be represented by the dot product of the corresponding vectors.  This dot product is computed by first applying the corresponding basis matrix to the two vectors.  The result of this computation is a number.  This number is called the inner product of the two vectors.  The inner product of two vectors can also be represented by a dot product of the two Gram matrices of the basis vectors.  The Gram matrix of a basis is a square matrix with the same dimensions as the basis.  The inner product of two vectors is the dot product of the corresponding vectors and the basis.  The inner product of two vectors can also be represented by a dot product of the corresponding Gram matrices.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
